{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98e13d8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-21T17:36:37.414221Z",
     "iopub.status.busy": "2025-04-21T17:36:37.413946Z",
     "iopub.status.idle": "2025-04-21T17:39:01.807008Z",
     "shell.execute_reply": "2025-04-21T17:39:01.806028Z"
    },
    "papermill": {
     "duration": 144.397843,
     "end_time": "2025-04-21T17:39:01.808323",
     "exception": true,
     "start_time": "2025-04-21T17:36:37.410480",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mRequirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\r\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement FluxFillPipeline (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for FluxFillPipeline\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 17:38:29.260595: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745257109.699601      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745257109.830850      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\r\n",
      "Using device: Tesla T4\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/319968455.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 1. Object Detection with DETR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Load background image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mbg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter URL or local path of background image: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mbackground\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \"\"\"\n\u001b[1;32m   1173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_stdin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m             raise StdinNotImplementedError(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install diffusers transformers accelerate --quiet\n",
    "!pip install opencv-python-headless pillow --quiet\n",
    "!pip install diffusers  FluxFillPipeline\n",
    "import torch\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from diffusers import FluxFillPipeline\n",
    "from diffusers.utils import load_image\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "!pip install -U huggingface_hub\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\") \n",
    "# Check GPU availability (T4 on Kaggle)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {torch.cuda.get_device_name(0) if device.type=='cuda' else 'CPU'}\")\n",
    "\n",
    "# 1. Object Detection with DETR\n",
    "# Load background image\n",
    "bg_path = input(\"Enter URL or local path of background image: \")\n",
    "background = Image.open(bg_path).convert(\"RGB\")\n",
    "\n",
    "# Load DETR processor and model\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\").to(device)\n",
    "\n",
    "# Detection loop with highest score selection\n",
    "best_score = 0\n",
    "best_det = None\n",
    "num_attempts = 3\n",
    "for attempt in range(num_attempts):\n",
    "    print(f\"Attempt {attempt + 1}/{num_attempts}\")\n",
    "    tensor_inputs = processor(images=background, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**tensor_inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([background.size[::-1]]).to(device)\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.5)[0]\n",
    "\n",
    "    # Collect detections\n",
    "    detections = []\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        detections.append({\n",
    "            'label': model.config.id2label[label.item()],\n",
    "            'score': round(score.item(), 3),\n",
    "            'box': [round(v, 1) for v in box.tolist()]\n",
    "        })\n",
    "\n",
    "   \n",
    "    background.save(\"background_detected.png\")\n",
    "    # hatha test na7ih ba3ed \n",
    "    \n",
    "\n",
    "    for det in detections:\n",
    "        if det['score'] > best_score:\n",
    "            best_score = det['score']\n",
    "            best_det = det\n",
    "\n",
    "if not best_det:\n",
    "    raise ValueError(\"No objects detected in top attempts.\")\n",
    "\n",
    "# Create mask for inpainting\n",
    "x0, y0, x1, y1 = map(int, best_det['box'])\n",
    "mask = np.zeros(background.size[::-1], dtype=np.uint8)\n",
    "mask[y0:y1, x0:x1] = 255\n",
    "mask_img = Image.fromarray(mask)\n",
    "\n",
    "# 2. Inpainting (Removal) using FluxFillPipeline\n",
    "# Skip model load if repo is gated or errors\n",
    "try:\n",
    "    pipe = FluxFillPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-Fill-dev\", torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "    removal = pipe(\n",
    "        prompt=\"\",\n",
    "        image=background,\n",
    "        mask_image=mask_img,\n",
    "        height=background.size[1],\n",
    "        width=background.size[0],\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=50,\n",
    "        max_sequence_length=512,\n",
    "        generator=torch.Generator(device).manual_seed(0)\n",
    "    ).images[0]\n",
    "    removal.save(\"background_removed.png\")\n",
    "    print(\"Saved background with object removed: background_removed.png\")\n",
    "except Exception as e:\n",
    "    print(\"Inpainting model loading failed. Skipping inpainting.\", e)\n",
    "    background.save(\"background_removed.png\")\n",
    "\n",
    "# 3. Product Insertion\n",
    "# Load product image and transparency mask\n",
    "product_path = input(\"Enter URL or local path of product image (PNG with alpha): \")\n",
    "product = Image.open(product_path).convert(\"RGBA\")\n",
    "\n",
    "# Convert images for OpenCV blending\n",
    "bg_cv = cv2.imread(\"background_removed.png\")\n",
    "fg_rgba = cv2.imread(product_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Extract mask and RGB channels\n",
    "b, g, r, a = cv2.split(fg_rgba)\n",
    "fg_cv = cv2.merge((b, g, r))\n",
    "mask_fg = a\n",
    "\n",
    "# Compute center for seamlessClone (use bbox center)\n",
    "cx = x0 + (x1 - x0) // 2\n",
    "cy = y0 + (y1 - y0) // 2\n",
    "\n",
    "# Seamless cloning\n",
    "output = cv2.seamlessClone(\n",
    "    fg_cv, bg_cv, mask_fg, (cx, cy), cv2.NORMAL_CLONE\n",
    ")\n",
    "cv2.imwrite(\"final_composite.png\", output)\n",
    "print(\"Saved final composite: final_composite.png\")\n",
    "\n",
    "# End of Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0754ef6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Install dependencies\n",
    "!pip install requests pillow diffusers transformers accelerate opencv-python-headless --quiet\n",
    "\n",
    "\n",
    "!pip install -U diffusers \n",
    "import requests\n",
    "import urllib.parse\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from diffusers import FluxFillPipeline\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {torch.cuda.get_device_name(0) if device.type=='cuda' else 'CPU'}\")\n",
    "\n",
    "# === 1. User Inputs ===\n",
    "product_name = input(\"Enter product name: \")\n",
    "product_desc = input(\"Enter product description: \")\n",
    "text_model = input(\"Choose text model (e.g. openai, qwen-coder): \")\n",
    "image_model = input(\"Choose image model (e.g. flux, midijourney): \")\n",
    "width = input(\"Enter background width (e.g. 1080): \")\n",
    "height = input(\"Enter background height (e.g. 1080): \")\n",
    "seed = input(\"Enter seed (blank for random): \") or None\n",
    "\n",
    "# === 2. Build and Enhance Prompt ===\n",
    "system_directives = (\n",
    "    \"You are TTIPEIG, the Ultimate Text-to-Image Prompt Enhancer and Image Generator. \"\n",
    "    \"Your task is to craft a vivid, detailed prompt that instructs an image API to create a scene \"\n",
    "    \"into which a product can be seamlessly inserted.\"\n",
    ")\n",
    "pre_prompt = f\"{system_directives} Product Name: {product_name}. Description: {product_desc}.\"\n",
    "enhance_url = (\n",
    "    \"https://text.pollinations.ai/\" + urllib.parse.quote(pre_prompt) +\n",
    "    f\"?model={text_model}\"\n",
    ")\n",
    "print(f\"Enhancing prompt via text API (model={text_model})...\")\n",
    "resp = requests.get(enhance_url)\n",
    "resp.raise_for_status()\n",
    "enhanced_prompt = resp.text\n",
    "print(\"Enhanced prompt:\")\n",
    "print(enhanced_prompt)\n",
    "\n",
    "# === 3. Generate Background via Image API ===\n",
    "params = {\n",
    "    'model': image_model,\n",
    "    'width': width,\n",
    "    'height': height,\n",
    "    'seed': seed,\n",
    "    'nologo': 'true',\n",
    "    'private': 'false',\n",
    "    'enhance': 'true',\n",
    "    'safe': 'false'\n",
    "}\n",
    "query = '&'.join(f\"{k}={urllib.parse.quote(str(v))}\" for k,v in params.items() if v)\n",
    "image_url = f\"https://image.pollinations.ai/prompt/{urllib.parse.quote(enhanced_prompt)}?{query}\"\n",
    "print(\"Generating background scene:\", image_url)\n",
    "img_resp = requests.get(image_url)\n",
    "img_resp.raise_for_status()\n",
    "background = Image.open(BytesIO(img_resp.content)).convert(\"RGB\")\n",
    "background.save(\"background_generated.png\")\n",
    "print(\"Saved generated background: background_generated.png\")\n",
    "\n",
    "display(background)\n",
    "\n",
    "# === 4. Object Detection & Removal ===\n",
    "# Load DETR\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\").to(device)\n",
    "\n",
    "# Detection loop for top scoring object\n",
    "best_score = 0\n",
    "best_det = None\n",
    "num_attempts = 3\n",
    "for i in range(num_attempts):\n",
    "    print(f\"Detection attempt {i+1}/{num_attempts}\")\n",
    "    inputs = processor(images=background, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([background.size[::-1]]).to(device)\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs, target_sizes=target_sizes, threshold=0.5\n",
    "    )[0]\n",
    "    for score, label, box in zip(results['scores'], results['labels'], results['boxes']):\n",
    "        if score.item() > best_score:\n",
    "            best_score = score.item()\n",
    "            best_det = {'label': model.config.id2label[label.item()],\n",
    "                        'score': round(score.item(),3),\n",
    "                        'box': [round(v,1) for v in box.tolist()]}\n",
    "if not best_det:\n",
    "    raise ValueError(\"No object detected for removal.\")\n",
    "print(f\"Removing object: {best_det['label']} (score: {best_det['score']}) at {best_det['box']}\")\n",
    "\n",
    "# Create mask and inpaint\n",
    "x0,y0,x1,y1 = map(int, best_det['box'])\n",
    "mask = np.zeros(background.size[::-1], dtype=np.uint8)\n",
    "mask[y0:y1, x0:x1] = 255\n",
    "mask_img = Image.fromarray(mask)\n",
    "\n",
    "try:\n",
    "    pipe = FluxFillPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-Fill-dev\", torch_dtype=torch.float32\n",
    "    ).to(device)\n",
    "    removed = pipe(\n",
    "        prompt=\"\",\n",
    "        image=background,\n",
    "        mask_image=mask_img,\n",
    "        height=background.height,\n",
    "        width=background.width,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=50,\n",
    "        max_sequence_length=512,\n",
    "        generator=torch.Generator(device).manual_seed(0)\n",
    "    ).images[0]\n",
    "    removed.save(\"background_removed.png\")\n",
    "    print(\"Saved background after removal: background_removed.png\")\n",
    "    background = removed\n",
    "except Exception as e:\n",
    "    print(\"Inpainting failed; using original background.\", e)\n",
    "    background.save(\"background_removed.png\")\n",
    "\n",
    "# === 5. Product Insertion ===\n",
    "product_path = input(\"Enter local path of your product PNG with alpha (e.g. /mnt/data/product.png): \")\n",
    "fg = cv2.imread(product_path, cv2.IMREAD_UNCHANGED)\n",
    "bg_cv = cv2.imread(\"background_removed.png\")\n",
    "b, g, r, a = cv2.split(fg)\n",
    "fg_rgb = cv2.merge((b,g,r))\n",
    "mask_fg = a\n",
    "\n",
    "\n",
    "# Ensure foreground fits inside background\n",
    "bg_h, bg_w = bg_cv.shape[:2]\n",
    "fg_h, fg_w = fg_rgb.shape[:2]\n",
    "\n",
    "# Resize foreground if too big\n",
    "if fg_h > bg_h or fg_w > bg_w:\n",
    "    scale = min(bg_h / fg_h, bg_w / fg_w) * 0.5  # shrink to fit\n",
    "    new_size = (int(fg_w * scale), int(fg_h * scale))\n",
    "    fg_rgb = cv2.resize(fg_rgb, new_size, interpolation=cv2.INTER_AREA)\n",
    "    mask_fg = cv2.resize(mask_fg, new_size, interpolation=cv2.INTER_AREA)\n",
    "    fg_h, fg_w = fg_rgb.shape[:2]\n",
    "\n",
    "# Recalculate center so object doesn't overflow\n",
    "cx = min(max(fg_w//2, cx), bg_w - fg_w//2)\n",
    "cy = min(max(fg_h//2, cy), bg_h - fg_h//2)\n",
    "\n",
    "\n",
    "composite = cv2.seamlessClone(fg_rgb, bg_cv, mask_fg, (cx,cy), cv2.NORMAL_CLONE)\n",
    "cv2.imwrite(\"final_composite.png\", composite)\n",
    "print(\"Saved final composite: final_composite.png\")\n",
    "\n",
    "# Display final composite\n",
    "final = Image.open(\"final_composite.png\")\n",
    "display(final)\n",
    "\n",
    "# End of Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bcc9e9",
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-21T17:23:03.646Z",
     "iopub.execute_input": "2025-04-21T17:16:26.892149Z",
     "iopub.status.busy": "2025-04-21T17:16:26.891314Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kaggle Notebook: API‑Driven Background Generation + DETR Removal + Product Insertion\n",
    "\n",
    "# Install dependencies\n",
    "!pip install requests pillow diffusers transformers accelerate opencv-python-headless --quiet\n",
    "!pip install -U diffusers \n",
    "!pip install -U huggingface_hub\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\") \n",
    "import requests\n",
    "import urllib.parse\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from diffusers import FluxFillPipeline\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {torch.cuda.get_device_name(0) if device.type=='cuda' else 'CPU'}\")\n",
    "\n",
    "# === 1. User Inputs ===\n",
    "product_name = input(\"Enter product name: \")\n",
    "product_desc = input(\"Enter product description: \")\n",
    "text_model = input(\"Choose text model (e.g. openai, qwen-coder): \")\n",
    "image_model = input(\"Choose image model (e.g. flux, midijourney): \")\n",
    "width = input(\"Enter background width (e.g. 1080): \")\n",
    "height = input(\"Enter background height (e.g. 1080): \")\n",
    "seed = input(\"Enter seed (blank for random): \") or None\n",
    "\n",
    "# === 2. Build and Enhance Prompt ===\n",
    "system_directives = (\n",
    "    \"You are TTIPEIG, the Ultimate Text-to-Image Prompt Enhancer and Image Generator. \"\n",
    "    \"Your task is to craft a vivid, detailed prompt that instructs an image API to create a single object-centered scene \"\n",
    "    \"with only one product clearly featured, for easy model identification and integration.\"\n",
    ")\n",
    "pre_prompt = f\"{system_directives} Product Name: {product_name}. Description: {product_desc}.\"\n",
    "enhance_url = (\n",
    "    \"https://text.pollinations.ai/\" + urllib.parse.quote(pre_prompt) +\n",
    "    f\"?model={text_model}\"\n",
    ")\n",
    "print(f\"Enhancing prompt via text API (model={text_model})...\")\n",
    "resp = requests.get(enhance_url)\n",
    "resp.raise_for_status()\n",
    "enhanced_prompt = resp.text\n",
    "print(\"Enhanced prompt:\")\n",
    "print(enhanced_prompt)\n",
    "\n",
    "# === 3. Generate Background via Image API ===\n",
    "params = {\n",
    "    'model': image_model,\n",
    "    'width': width,\n",
    "    'height': height,\n",
    "    'seed': seed,\n",
    "    'nologo': 'true',\n",
    "    'private': 'false',\n",
    "    'enhance': 'true',\n",
    "    'safe': 'false'\n",
    "}\n",
    "query = '&'.join(f\"{k}={urllib.parse.quote(str(v))}\" for k,v in params.items() if v)\n",
    "image_url = f\"https://image.pollinations.ai/prompt/{urllib.parse.quote(enhanced_prompt)}?{query}\"\n",
    "print(\"Generating background scene:\", image_url)\n",
    "img_resp = requests.get(image_url)\n",
    "img_resp.raise_for_status()\n",
    "background = Image.open(BytesIO(img_resp.content)).convert(\"RGB\")\n",
    "background.save(\"background_generated.png\")\n",
    "print(\"Saved generated background: background_generated.png\")\n",
    "\n",
    "display(background)\n",
    "\n",
    "# === 4. Object Detection & Removal ===\n",
    "# Load DETR\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\").to(device)\n",
    "\n",
    "# Detection loop for top scoring object\n",
    "best_score = 0\n",
    "best_det = None\n",
    "num_attempts = 3\n",
    "for i in range(num_attempts):\n",
    "    print(f\"Detection attempt {i+1}/{num_attempts}\")\n",
    "    inputs = processor(images=background, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([background.size[::-1]]).to(device)\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs, target_sizes=target_sizes, threshold=0.5\n",
    "    )[0]\n",
    "    for score, label, box in zip(results['scores'], results['labels'], results['boxes']):\n",
    "        if score.item() > best_score:\n",
    "            best_score = score.item()\n",
    "            best_det = {'label': model.config.id2label[label.item()],\n",
    "                        'score': round(score.item(),3),\n",
    "                        'box': [round(v,1) for v in box.tolist()]}\n",
    "if not best_det:\n",
    "    raise ValueError(\"No object detected for removal.\")\n",
    "print(f\"Removing object: {best_det['label']} (score: {best_det['score']}) at {best_det['box']}\")\n",
    "\n",
    "# Create mask and inpaint\n",
    "x0,y0,x1,y1 = map(int, best_det['box'])\n",
    "mask = np.zeros(background.size[::-1], dtype=np.uint8)\n",
    "mask[y0:y1, x0:x1] = 255\n",
    "mask_img = Image.fromarray(mask)\n",
    "\n",
    "try:\n",
    "    pipe = FluxFillPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-Fill-dev\", torch_dtype=torch.float32\n",
    "    ).to(device)\n",
    "    removed = pipe(\n",
    "        prompt=\"\",\n",
    "        image=background,\n",
    "        mask_image=mask_img,\n",
    "        height=background.height,\n",
    "        width=background.width,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=50,\n",
    "        max_sequence_length=512,\n",
    "        generator=torch.Generator(device).manual_seed(0)\n",
    "    ).images[0]\n",
    "    removed.save(\"background_removed.png\")\n",
    "    print(\"Saved background after removal: background_removed.png\")\n",
    "    background = removed\n",
    "except Exception as e:\n",
    "    print(\"Inpainting failed; using original background.\", e)\n",
    "    background.save(\"background_removed.png\")\n",
    "\n",
    "# === 5. Product Insertion ===\n",
    "product_path = input(\"Enter local path of your product PNG with alpha (e.g. /mnt/data/product.png): \")\n",
    "fg = cv2.imread(product_path, cv2.IMREAD_UNCHANGED)\n",
    "bg_cv = cv2.imread(\"background_removed.png\")\n",
    "\n",
    "# Extract alpha mask and RGB\n",
    "b, g, r, a = cv2.split(fg)\n",
    "fg_rgb = cv2.merge((b,g,r))\n",
    "mask_fg = a\n",
    "\n",
    "# Resize foreground to match removed object's size\n",
    "target_w, target_h = x1 - x0, y1 - y0\n",
    "fg_rgb = cv2.resize(fg_rgb, (target_w, target_h), interpolation=cv2.INTER_AREA)\n",
    "mask_fg = cv2.resize(mask_fg, (target_w, target_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# Compute center for placement\n",
    "cx = x0 + target_w // 2\n",
    "cy = y0 + target_h // 2\n",
    "\n",
    "# Ensure placement is within bounds\n",
    "bg_h, bg_w = bg_cv.shape[:2]\n",
    "cx = min(max(target_w//2, cx), bg_w - target_w//2)\n",
    "cy = min(max(target_h//2, cy), bg_h - target_h//2)\n",
    "\n",
    "# Composite product into background\n",
    "composite = cv2.seamlessClone(fg_rgb, bg_cv, mask_fg, (cx, cy), cv2.NORMAL_CLONE)\n",
    "cv2.imwrite(\"final_composite.png\", composite)\n",
    "print(\"Saved final composite: final_composite.png\")\n",
    "\n",
    "# Display final composite\n",
    "final = Image.open(\"final_composite.png\")\n",
    "display(final)\n",
    "\n",
    "# End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7211952,
     "sourceId": 11503109,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7212155,
     "sourceId": 11503381,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7212223,
     "sourceId": 11503468,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7212386,
     "sourceId": 11503665,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 154.210062,
   "end_time": "2025-04-21T17:39:05.260912",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-21T17:36:31.050850",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
